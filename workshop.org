#+SEQ_TODO: TODO | DONE

#+HUGO_BASE_DIR: ./site

* Table of content
  :PROPERTIES:
  
  :END:
** Introduction
  :PROPERTIES:
  :EXPORT_FILE_NAME: _index
  :EXPORT_HUGO_SECTION: introduction
  :END:
   What we are concerned about today: the problem(s) we want to solve.
   What we are going to do, the tools that we are going to use.
   The example that we are going to consider.
** Making a package
  :PROPERTIES:
  :EXPORT_CUSTOM_FRONT_MATTER: :chapter "true"
  :EXPORT_FILE_NAME: _index
  :EXPORT_HUGO_SECTION: part1_making_a_package
  :END:

** Reusing a package across analyses
  :PROPERTIES:
  :EXPORT_CUSTOM_FRONT_MATTER: :chapter "true"
  :EXPORT_FILE_NAME: _index
  :EXPORT_HUGO_SECTION: part2_reusing_a_package
  :END:
** Virtual environments
  :PROPERTIES:
  :EXPORT_CUSTOM_FRONT_MATTER: :chapter "true"
  :EXPORT_FILE_NAME: _index
  :EXPORT_HUGO_SECTION: intermezzo_virtual_environments
  :END:
* glossary
- package
- project
- analysis
- virtual environement

* Introduction
  :PROPERTIES:
  :EXPORT_FILE_NAME: _index
  :EXPORT_HUGO_SECTION: introduction
  :END:

  In this workshop, you are going to learn how to organise your Python software into
  /packages/. Doing so, you will be able to 
  - Have your software clearly organised in a way standard among Python developpers, making
    your software easier to understand, test and debug. 
  - Reuse your code across yout research projects and analyses. No more copying and pasting
    blocks of code around: implement and test things once.
  - Easy share your software, making everybody (including yourself) able to ~pip install~
    your package!

  The *plan* is the following: we are going to start from a couple of rather messy python scripts and gradually
  transform them into a full-blown python package. At the end of this workshop, you'll know:
  - What is a Python package and how to create one (and why!).
  - How to share your packages across several of your projects.
  - maintain this package independantly from your research projects and analyses.
  - What are virtual environments and how to use them to install different versions of a package
    for different analyses.
  - How to share your package on the [[https://pypi.org/][Python Package Index]] (PyPI), effectively making it straightforward
    for anyone to install your package with the ~pip~ package manager (and much more!).
  - Where to go next.

  Sounds interesting? Good! Get a cup of your favorite beverage and let's get started.

** Materials for this course

   This course assumes that you have a local copy of the materials repository.
   To make it, you can simply clone the repository using git:
   #+begin_src shell
   git clone https://github.com/OxfordRSE/python-packaging-course
   #+end_src

   For non-git users, you can visit https://github.com/OxfordRSE/python-packaging-course 
   and download the materials as a ZIP archive ("code" green button on the top right corner).

** Two scripts to analyse a timeseries
   
   Our starting point for this workshop is the scripts ~analysis.py~ and ~show_extremes.py~.
   You'll find them at the root of the course directory.

   Both scripts perform operations on a /timeseries/, a sequence of numbers indexed by time.
   This timeseries is located in ~analysis1/data/brownian.csv~ and describes the (simulated)
   one-dimensional movement of a particle undergoing [[https://en.wikipedia.org/wiki/Brownian_motion][brownian motion]].
#+begin_src
0.0,-0.2709970143466439
0.1,-0.5901672546059646
0.2,-0.3330040851951451
0.3,-0.6087488066987489
0.4,-0.40381970872171624
0.5,-1.0618436436553174
...
#+end_src
   The first column contains the various times when the particle's position was recorded, and
   the second column the corresponding position.

   Let's have a quick overview of these scripts, but *Don't try to understand the details*, it is irrelevant to the present workshop. 
   Instead, let's briefly describe their structure.
   

*** Overview of ~base.py~
    
    After reading the timeseries from the file ~brownian.csv~, this script ~base.py~ does 
    three things:
    - It computes the average value of the particle's position over time and the standard 
      deviation, which gives a measure of the spread around the average value.
    - It plots the particle's position as a function of time from the initial time until
      50 time units.
    - Lastly, it computes and plots the histogram of the particle's position over the entirety
      of the timeseries. In addition, the theoritical histogram is computed and drawn as a
      straight line on top of the measured histogram. For this, a function ~get_theoritical_histogram~
      is defined, resembling the the ~numpy~ function ~histogram~.

    You're probably familiar with this kind of script, in which several independant operations are performed
    on a single dataset. 
    It is the typical output of some "back of the enveloppe", exploratory work so common in research.
    Taking a step back, these scripts are the reason why high-level languages like Python are so popular
    among scientists and researchers: got some data and want to quickly get some insight into it? Let's 
    just jot down a few lines of code and get some numbers, figures and... ideas!

    Whilst great for some short research phases, this "back of the enveloppe scripting" way of working can quickly 
    backfire if maintained over longer period of time, perhaps even over your whole research project.
    Going back to ~base.py~, consider the following questions:
    - What would you do if you wanted to plot the timeseries over the last 50 time units instead of the first 50?
    - What would you do if you wanted to visualise the /Probablity Density Function/ instead of the histogram (effectively passing the optional argument src_python{density=true}
      to ~numpy.histogram~).
    - What would you do if you were given a similar dataset to ~brownian.csv~ and asked to compute the mean, compute the histogram along with other things not implemented in ~base.py~ ?
    
    In the interest of time, you are likely to end up modifying some specific lines (to compute the PDF instead of the histogram for example), or/and copy and paste of lot of code.
    Whilst convenience on a short term basis, is it going to be increasingly difficult to understand your script, track its purpose, and test that its results are correct.
    Three months later, facing a smilar dataset, would you not be tempted to rewrite things from scratch? It doesn't have to be this way! As you're going to learn in this ourse, 
    organising your Python software into /packages/ alleviates most of these issues.
    
*** Overview of ~show_extremes.py~

    Contrarily to ~base.py~, the script ~show_extreme.py~ has one purpose: to
    produce a figure displaying the full timeseries (the particle's position as a function
    of time from the initial recorded time to the final recorded time) and to hightlight
    ~extreme fluctuations~: the rare events when the particle's position is above a given 
    value ~threshold~.

    The script starts by reading the data and setting the value of the threshold:
    #+begin_src python
      timeseries = np.genfromtxt("./data/brownian.csv", delimiter=",")
      threshold = 2.5
    #+end_src

    The rest of the script is rather complex and its discussion is irrelevant to this course.
    Let's just stress that it exhibits the same pitfalls than ~base.py~.


** Separating methods from parameters and data
   :PROPERTIES:
   :EXPORT_HUGO_SECTION: part1_making_a_package
   :EXPORT_FILE_NAME: separating_methods_from_parameters_and_data
   :EXPORT_HUGO_WEIGHT: auto
   :END:

Roughly speaking, a numerical experiment is made of three components:
- The data (dataset, or parameters of simulation)
- The operations performed on this data
- The output (numbers, plots)

As we saw, scripts ~base.py~, and ~show_extremes.py~ mix the three above components into a single 
~.py~ file, making the analysis difficult (sometimes even risky!) to modify and test. 
Re-using part of the code means copying and pasting blocks of code out of their original context, which is
a dangerous practice.

In both scripts, the operations performed on the timeseries ~brownian.csv~ are independant from it, and could very well
be applied to another timeseries. In this workshop, we're going to extract these operations (computing the mean, the histogram, visualising the extremes...),
and formulate them as Python /functions/, grouped by theme inside /modudes/, in a way that can be reused across similar analyses. We'll then bundle these modules into a Python
/package/ that will make it straightfoward to share them across different analysis, but also other people.


A script using our package could look like this:
#+begin_src python
  import numpy as np
  import matplotlib.pyplot as plt
  import my_pkg

  timeseries = np.genfromtxt("./data/my_timeseries.csv", delimiter=",")

  mean, var = my_pkg.get_mean_and_var(timeseries)

  fig, ax = my_pkg.get_pdf(timeseries)

  threshold = 3*np.sqrt(var)
  fig, ax = my_pkg.show_extremes(timeseries, threshold)
#+end_src

Compare the above to ~base.py~: it is much shorter and easier to read.
The actual implementation of the various operations (computing the mean and variance, computing the histogram...) is now
/encapsulated/ inside the package ~my_pkg~. 
All that remains are the actual steps of the analysis.

If we were to make changes to the way some operations are implemented, we would simply make
changes to the package, leaving the scripts unmodified. This reduces the risk of messing of introducing errors in your analysis, when all what you want to do is modyfying
some opearation of data.
The changes are then made available to all the programs that use the package: no more copying and pasting code around.

Taking a step back, the idea of separating different components is pervasive in software developemt
and software design. Different names depending on the field (encapsulation, separation of concerns,
bounded contexts...).

* Making a python package
** From scripts to modules
  :PROPERTIES:
  :EXPORT_FILE_NAME: from_scripts_to_modules
  :EXPORT_HUGO_SECTION: part1_making_a_package
  :EXPORT_HUGO_WEIGHT: auto
  :END:
*** Functions, modules, packages
- functions, classes
#+begin_src python
  # operations.py
  def add(a,b):
      return a+b
#+end_src
- modules
  Collection of python objects (classes, functions, variables)
#+begin_src python
  from operations import add
  # "From file (or module) operations.py import object add"

  result = add(1,2)
#+end_src
- packages
  Collection of modules (~.py~ files)
#+begin_src python
  from calculator.operations import add
  from calculator.representations import hexa

  a = hexa(1)
  b = hexa(2)

  result = add(a,b)
#+end_src

*** Activity 1 - Turning scripts into a collection of functions
 Let's rewrite both scripts ~scripts/analysis.py~ and ~scripts/show_extremes.py~
 as a collection of functions that can be reused in separate scripts.

The directory ~analysis1/tstools/~ contains 3 python modules that contain (incomplete) functions performing 
the same operations on data described in the original scripts ~analysis.py~ and ~show_extremes.py~

#+begin_example
  python-packaging-workshop/
	  scripts/
	  analysis1/
		  tstools/
			  __init__.py
			  moments.py
			  vis.py
			  extremes.py
#+end_example

1. Open ~moments.py~ and complete function ~get_mean_and_var~ (replace the
   string ~"######"~).
2. Open file ~vis.py~ and complete functions ~plot_trajectory_subset~ and
   ~plot_histogram~ (replace the strings ~"######"~).
*Hint*: Use ~scripts/analysis.py~ as a reference.

The file ~tstools/extremes.py~ implements a function ~show_extremes~ corresponding to script ~show_extremes.py~.
It is already complete.

** The ~tstools~ package
  :PROPERTIES:
  :EXPORT_FILE_NAME: the_tstools_package
  :EXPORT_HUGO_SECTION: part1_making_a_package
  :EXPORT_HUGO_WEIGHT: auto
  :END:
We now have a ~tstools~ directory with 3 modules:
#+begin_example
  analysis1/
	  tstools/
		  __init__.py
		  moments.py
		  vis.py
		  show_extremes.py
	  data/
#+end_example

In way, the directory ~tstools~ is already a pacakge, in the sens that it is possible to import each functions from the modules:
#+begin_src python :exports code
import tstools.moments
from tstools.vis import plot_histogram

timeseries = np.genfromtxt("./data/brownian.csv", delimiter=",")

mean = tstools.moments.get_mean(timeseries)
fig, ax = tstools.moments.plot_histogram(timeseries)
#+end_src

Let's try to import the package as a whole:
#+begin_src python :dir ".infrastructure/tstools-no-init" :results output :exports code
  # compute-mean.py
  import numpy as np
  import tstools
  timeseries = np.genfromtxt("./data/brownian.csv", delimiter=",")
  mean = tstools.moments.get_mean(timeseries)
#+end_src

#+begin_example
$ python compute_mean.py
Traceback (most recent call last):
  File "<stdin>", line 4, in <module>
AttributeError: module 'tstools' has no attribute 'moments'
#+end_example

What happened here? When importing the directory ~tstools~, the python interpreter
looks for a file named ~__init__.py~ inside this directory and imports this python file.
If this python file is empty, or simply doesnt exists... nothing is imported.

In the following section we add some ~import~ statements into the ~__init__.py~ so that 
all our functions (in the three modules) ar avaialbe under the single namespae ~tstools~.
** init dot pie
  :PROPERTIES:
  :EXPORT_FILE_NAME: init_dot_pie
  :EXPORT_HUGO_SECTION: part1_making_a_package
  :EXPORT_HUGO_WEIGHT: auto
  :END:

Whenever you import a directory, Python will look for a file __init__.py at the root of this
directory, and, if found, will import it.
It is the presence of this initialization file that truly makes the ~tstools~ directory a Python
package[fn:1].

As a first example, let's add the following code to ~__init__.py~:
#+begin_src python
  # tstools/__init__.py
  from os.path import basename
  filename = basename(__file__)
  print(f"Hello from {filename}")
#+end_src

If we now import the ~tstools~ package:
#+begin_src python :dir ".infrastructure/simple_init_dot_pie" :results output :exports both
  import tstools
  print(tstools.filename)
#+end_src

#+RESULTS:
: Hello from __init__.py
: __init__.py

The lesson here is that any object (variable, function, class) defined in the ~__init__.py~ file is available
under the package's namespace.

*** Activity 2 - Bringing all functions under a single namespace
Our package isn't very big, and the internal strucure with 3 different modules isnt
very relevant for a user.

*Write* the ~__init__.py~ so that all functions defined in
modules ~tstools.py~ and ~show_extremes.py~ are accessible directly
at the top-lvel (under the ~tstools~ namespace), /i.e/

#+begin_src python
  import tstools
  mean, var = tstools.get_mean_and_var(timeseries) # instead of mean, var = tstools.moments.get_mean_and_var(...)
  fig, ax = tstools.show_extremes(timeseries, 4*np.sqrt(var)) # instead of fig, ax = tstools.vis.show_extremes(...)
#+end_src

*Hint*: By default python looks for modules in the current directory
and some other locations (more about that later). When using ~import~,
you can refer to modules in the current package using the /dot notation/:
#+begin_src python
  # import something from module that resides
  # in the current package (next to the __init__.py)
  from .module import something
#+end_src

*** Using the package

Our package is now ready to be used in our analysis, and a analysis scripts could look like this:

#+begin_src python
  # analysis1/analysis1.py
  import numpy as np
  import matplotlib.pyplot as plt
  import tstools

  timeseries = np.genfromtxt("./data/my_timeseries.csv", delimiter=",")

  mean, var = tstools.get_mean_and_var(timeseries)

  fig, ax = tstools.get_pdf(timeseries)

  threshold = 3*np.sqrt(var)
  fig, ax = tstools.show_extremes(timeseries, threshold)
#+end_src

Note that the above does the job for both scripts ~scripts/analysis.py~ and ~scripts/show_extremes.py~! Much better don't you think?
*** TODO Whats the value of any empty ~__init__.py~ ?              :noexport:
*** Note: objets defined in __init__.py are avaialbe when importing /the pacakge/ :noexport:
#+begin_src python
    # __init__.py
    mysymbol = "something"
    print(mysymbol)
#+end_src

#+begin_src python
  from tstools.tstools import get_mean_and_var
  # this prints "something" but mysymbol is not
  # accessible from tstools' namespace
#+end_src
* Part 2 - using the package across analyses
** Another analysis
   :PROPERTIES:
   :EXPORT_HUGO_WEIGHT: auto
   :EXPORT_HUGO_SECTION: part2_reusing_a_package
   :EXPORT_FILE_NAME: another-analysis
   :END:
Let's say that we have another directory ~analysis2~, that contains another
but similar dataset to ~analysis1/data/brownian.csv~.
Now that we've structured our software into a python package, we would like
to reuse that package for our second analysis.

Let us simply write a python scripts ~analysis2.py~, that imports the ~tstools~ package
created in the previous section.
#+begin_example
  analysis2/
	  analysis2.py
	  data/
		  timeseries.csv
#+end_example

#+begin_src python :dir "analysis2" :exports code
  # analysis2/analysis2.py
  import numpy as np
  import tstools

  timeseries = np.genfromtxt("./data/data_analysis2.csv")
  fig, ax = tstools.plot_trajectory_subset(timeseries, 0, 50, dt=0.1)
#+end_src

#+begin_example
$ python analysis2.py
Traceback (most recent call last):
  File "<stdin>", line 10, in <module>
  File "<stdin>", line 5, in main
ModuleNotFoundError: No module named 'tstools'
#+end_example

At the moment lives in the directory ~analysis1/~, and, unfortunately, Python cannot find it!
How can we tell Python where our package is?

** Where does python look for packages?
   :PROPERTIES:
   :EXPORT_HUGO_WEIGHT: auto
   :EXPORT_HUGO_SECTION: part2_reusing_a_package
   :EXPORT_FILE_NAME: where-does-python-look-for-packages
   :END:
When using the ~import~ statement, the python interpreter looks for the package (or module) in a list of directories
known as the /python path/.

Let's find out about what directories constitute the python path:

#+begin_example
$ python
>>> import sys
>>> sys.path
#+end_example

The order of this list matters: it is the order in which python looks into the directories
that constitute the python path.
To begin with, Python first looks in the current directory.
If the package/module isn't found there, the python intepreter looks in the following directories
(in this order):
- ~/usr/lib/python38.zip~
- ~/usr/lib/python3.8~
- ~/usr/lib/python3.8/lib-dynload~
The above contain the modules and packages in the /standard library/, /i.e/ the packages and modules that
come "pre-installed" with Python.
Finally, the python interpreter looks inside the directory ~python-workshop/lib/python3.8/site-packages/~.

For Python to find out package ~tstools~ it must be located in one of the directories listed in
the ~sys.path~ list. If it is the case, the package is said to be /installed/.

Looking back at the example in the [[* Another analysis][previous section]], let's list some potential workarounds
for the ~tstools~ package to be importable in ~analysis2/~.:

1. *Copy (~analysis1/tstools/~) in ~analysis2/~*.
  You end up with two independant packages. If you make changes to one, you have to remember to make the same
  changes to the other. It's the usual copy and paste problems: inefficient and error-prone.
2. *Add ~analysis1/~ to ~sys.path~*.
  At the beginning of ~analysis2.py~, you could just add
  #+begin_src python
    import sys
    sys.path.append("../analysis1/")
  #+end_src
  This approach can be sufficient in some situations, but generally not recommended. What if the package directory is relocated?
3. *Copy ~analysis1/tstools~ directory to the ~site-packages/~ directory.*
  You have to know where the ~site-packages~ is. This depends on your current system and python environment (see below).
  The location on your macine may very well be differnt from the location on your colleague's machine.

More generally, the three above approaches overlook a very important point: *dependencies*.
Our package has two: numpy and matplotlib.
If you were to give your package to a colleague, nothing guarantees that they have both packages installed.
This is a pedagogical example, as it is likely that they would have both installed, given the popularity of these packages.
However, if your package relies on less widespread packages, specific versions of them or maybe a long list of packages,
it is important to make sure that they are available.

Note that all three above approaches work. 
However, unless you have a good reason to use one of them, they are not recommended for the
reasons above. In the next section, we look at the recommended way to install a package, using
~setuptools~ and ~pip~.

** setuptools, setup dot pie and pip
   :PROPERTIES:
   :EXPORT_HUGO_WEIGHT: auto
   :EXPORT_HUGO_SECTION: part2_reusing_a_package
   :EXPORT_FILE_NAME: setuptools-and-setup-do-_pie
   :END:
The recommended way to install a package is to use the ~setuptools~ library in conjunction with
~pip~, the official python /package manager/.
Effectively, this approach is roughly equivalent to copying the package to the ~site-packages~ directory,
expect that the process in *automated*.

*** pip
Pip is the de facto package manager for Python packages. 
It's main job is to install, remove, upgrade, configure and manage Python packages, both available
locally on your machine but also hosted on on the [[https://pypi.org/][Python Package Index (PyPI)]].
Pip is maintained by the [[https://www.pypa.io/en/latest/][Python Packaging Authority]].

Installing a package with ~pip~ looks like this
#+begin_src shell
pip install <package directory>
#+end_src

let's give it a try
#+header: :prologue "exec 2>&1" :epilogue "true"
#+header: :results output :exports both
#+begin_src shell :dir  ".infrastructure/pip_example_no_setup" 
# In directory analysis1/
pip install ./tstools
#+end_src

#+RESULTS:
: ERROR: Directory './tstools' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.

The above doesn't really look like our package got installed properly.
For ~pip~ to be able to install our package, we must first give it some information about it.
In fact, ~pip~ expects to find a python file named ~setup.py~ in the directory that it is
given as an argument. This file will contain some metadata about the package and tell ~pip~
the location of the actual source of the package.

*** ~setup.py~ (setup dot pie) and distribution packages

The ~setup.py~ file is a regular Python file that makes a call to the ~setup~ function
available in the ~setuptools~ package.

Let's have a look at a minimal ~setup.py~ file for our ~tstools~ package:
#+begin_src python
  from setuptools import setup

  setup(name='tstools',
	version='0.1',
	description='A package to analyse timeseries',
	url='myfancywebsite.com',
	author='Spam Eggs',
	package=['tstools'],
	license='GPLv3')
#+end_src

The above gives ~pip~ some metadata about our package: its version, a short description,
its authors, ad its license.
In addition, it gives ~setup~ the location of the package to be installed, in this case
the directory ~tstools~.

*IMPORTANT*: The above ~setup.py~ states ~(...,package=["tstools"],...)~.
In English, this means "setuptools, please install the package ~tstools/~ located in the same directory as the file ~setup.py~".
This therefore assumes that the file ~setup.py~ resides in the directory that /contains/ the package, in this case ~analysis1/~.
#+begin_example
  python-workshop/
	  analysis1/
		  data/
		  analysis1.py
		  setup.py
		  tstools/
#+end_example

Actually, there are no reasons for our ~tstools~ package to be located in the ~analysis1/~ directory.
Indeed, the package is independant from this specific analysis, and we want to share it among multiple analyses.

To reflect this, let's move the ~tstools~ package into a new directory ~tstools-dist~ located next to the ~anaylis1~ and
~analysis2~ directories:

#+begin_example
  python-workshop/
	  analysis1/
		  data/
		  analysis1.py
	  analysis2/
		  data/
		  analysis2.py
	  tsools-dist/
		  setup.py
		  tstools/
#+end_example

The directory ~tstools-dist~ is a /distribution package/, containing the ~setup.py~ file and the package itself - the ~tstools~ directory.
These are the two minimal ingredients required to /distribute/ a package, see section ??.

*** Activity 3 -  Installing ~tsools~ with pip
1. Write a new ~setup.py~ file in directory ~tstools-dist~ including the following metadata:
  + The name of the package (could be ~tstools~ but also could be anything else)
  + The version of the package (for example 0.1)
  + A one-line description
  + Your name as the author
  + Your email
  + The GPLv3 license
  Hint: A list of optional keywords for ~setuptools.setup~ can be found [[https://setuptools.readthedocs.io/en/latest/setuptools.html#new-and-changed-setup-keywords][here]].
2. Uninstall numpy and matplotlib
   #+begin_src shell
     pip uninstall numpy matplotlib
   #+end_src
3. Install the ~tstools~ package with ~pip~.
   Remember: ~pip install <location of setup file>~
   Notice how ~numpy~ and ~matplotlib~ are automatically downloaded (can you find from where?)
   and installed.
4. Move to the directory ~analysis2/~ and check that you can import your package from there.
   Where is this package located?
   Hint: You can check the location a package using the ~__file__~ attribute.
5. The directory ~analysis2~ contains a timeseries under ~data/~. What is the average value
   of the timeseries?

Congratulations! Your ~tstools~ package is now installed can be reused across your analyses...
no more dangerous copying and pasting!

** Maintaining your package indepently from the anaylises that use it
   :PROPERTIES:
   :EXPORT_HUGO_WEIGHT: auto
   :EXPORT_HUGO_SECTION: part2_reusing_a_package
   :EXPORT_FILE_NAME: maintaining-your-pkg-independantly-from-your-analysis
   :END:
In the previous section you made your package "pip installable" by creating a ~setup.py~ file.
You then installed the package, effectively making accessible between different analysis directories.

However, a package is never set in stone: as you work on your analyses, you will almost certainly likely make changes to it,
for instance to add functionalities or to fix bugs.

You could just reinstall the package each time you make a modification to it, but this
obviously becomes tedious if you are constantly making changes (maybe to hunt down a bug) and/or testing your package.
In addition, you may simply forget to reinstall your package, leading to potentially very frustrating and time-consuming errors.

*** Editable installs
~pip~ has the ability to install the package in a so-called "editable" mode.
Instead of copying your package to the package installation location, pip will just
write a link to your package directory.
In this way, when importing your package, the python interpreter is redirected to
your package project directory.

To install your package in editable mode, use the ~-e~ option for the ~install~ command:
#+begin_src shell
pip install -e .
#+end_src

*** Actvity 4 - Editable install
1. Uninstall the package with ~pip uninstall tstools~
2. List all the installed packages and check that ~tstools~ is not among them
   Hint: Use ~pip --help~ to get alist of available ~pip~ commands.
3. re-install ~tstools~ in editable mode.
4. Modify the ~tstools.vis.plot_trajectory_subset~ so that it returns the maximum value
   over the trajectory subset, in addition to the ~figure~ and ~axis~.
   Hint: You can use the numpy function ~amax~ to find the maximum of an array.
5. What is the maximum value of the timeseries in ~analysis1/data/timeseries1.csv~ between
   t=0 and t = 4 ?

In editable mode, ~pip install~ just write a file ~<package-name>.egg-link~ at the package
installation location in place of the actual package. This file contains the location of the
package in your package project directory:

#+begin_src shell
cat ~/python-workshop-venv/lib/python3.8/site-packages/tstools.egg-link
/home/thibault/python-packaging-workshop/tstools
#+end_src

** Summary and break
   :PROPERTIES:
   :EXPORT_HUGO_WEIGHT: auto
   :EXPORT_HUGO_SECTION: part2_reusing_a_package
   :EXPORT_FILE_NAME: summary_and_break
   :END:
   - In order to reuse our package across different analyses, we must /install/ it.
     In effect, this means copying the package into a directory that is in the python path.
     This shouldn't be done manually, but instead using the ~setuptools~ package to write a
     ~setup.py~ file that is then processed by the ~pip install~ command.
   - It would be both cumbersome and error-prone to have to reinstall the package each time
     we make a change to it (to fix a bug for instance). Instead, the package can be installed
     in "editable" mode using the ~pip install -e~ command. This just redirects the python
     interpreter to your project directory.
   - The main value of packaging software is to faciliate its reuse across different projects.
     One you have extracted the right operations into a package that is independant of your
     analysis, you can easily "share" it between projects. In this way you avoid innefficient
     and dangerous duplication of code.

Beyond greatly facilitating code reuse, writing a python package (as opposed to a loosely
organised collection of modules) enables a clear organisation of your software into modules
and possibly subpackages. It makes it much easier for others, as well as yourself, to
understand the structure of your software, /i.e/ what-does-what.

Moreover, organising your python software into a package gives you access to a myriad
of fantastic tools used by thousands of python developers everyday. Examples include
pytest for automated testing, sphinx for building you documentation, tox for automation
of project-level tasks.

Next, we'll talk about python virtual environments. But before, fancy a little break?

[[/kisspng-cafe-coffee-cup-tea-cafe-graphic-5ac8dcf5aa0815.5906502615231132056965.png]]
* Intermezzo: Python virtual environments
** Installing different versions of a package
   :PROPERTIES:
   :EXPORT_HUGO_WEIGHT: auto
   :EXPORT_HUGO_SECTION: intermezzo_virtual_environments
   :EXPORT_FILE_NAME: installing_different_versions_of_a_package
   :END:
In the previous section you learned how to share a package across several projects, or analyses.
However, as your package and analyses evolve asynchronously, it is likely that you will reach a point when
you'd like differnet analyses to use different versions of your package, or different versions of  third-party
packages that your analysis rely on.

The question is then: /how to install two different versions of a same package?/
And the (short) answer is: *you cannot*.

If you type ~pip install numpy==1.18~, ~pip~ first looks for a version
of ~numpy~ already installed (in the ~site-packages/~ directory).
If it finds a different version, say 1.19, ~pip~ will uninstall it and
install numpy 1.18 instead.

This limitation is very inconvenient, and is the /raison d'être/ for virtual environments, which we disuss next.
** Virtual environments
   :PROPERTIES:
   :EXPORT_HUGO_WEIGHT: auto
   :EXPORT_HUGO_SECTION: intermezzo_virtual_environments
   :EXPORT_FILE_NAME: virtual_environments
   :END:

Roughly speaking, the python executable ~/some_dir/lib/pythonX.Y/bin/python~
and the package installation location ~/some_dir/lib/pythonX.Y/site-packages/~
consitute what is commonly referred to as the /python environment/.

If you cannot install different versions of a package in a single environment,
let's have multiple environments! This is the core idea of /python virtual environments/.
Whenever a python virtual environment ~my_env~ is /activated/, the ~python~ command points to a
python executable that is unique to this environment (~my-env/lib/pythonX.Y/bin/python~), with a unique package installation location
specific to this environment (~my_env/lib/pythonX.Y/site-packages~).

*** Activity 5 - Virtual environments

1. Move to the ~analysis1/~ directory and create a virtual-environment there:
  #+begin_src shell :exports code
    cd python-packaging-workshop/analysis1/
    python -m venv venv-analysis1
  #+end_src
  This commands creates a new directory ~analysis_venv~ in the current directory.
  Feel free to explore its contents.

2. Activate the virtual envoronment for analysis1
  #+begin_src shell :exports code
    source venv-analysis1/bin/activate
  #+end_src

3. What is the location of the current python executable?
   Hint: The built-in python package ~sys~ provides a variable ~executable~.

4. Use ~pip list~ to list the currently installed packages.
   Note that our package and its dependencies have disappeared, and only
   the core python packages are installed. We have a "fresh" python environment.

5. Move to the the ~tstools~ package project directory and install it into the
   current envirinment:
  #+begin_src shell :exports code
    pip install .
  #+end_src

6. Where was the package installed?
   Hint: When importing package ~package~ in python, use ~package.__file__~
   to check the location of the corresponding ~__init__.py~ file.


The above exercise demonstrates that, after activating the ~venv-analysis1~, the command ~python~
executes the python executable ~ analysis1/venv-analysis1/bin/python~, and python packages are installed
in the ~analysis1/venv-analysis1/lib/pythonX.Y/site-packages~ directory.
This means that we are now working in a python environment that is /isolated/ from other python environments
in your machine:
- other virtual environments
- system python environment (see below)
- other versions of python installed in your system
- Anaconda environments

You can therefore install all the packages necessery to your projects, without worry of breaking
other projects.

** Make virtual environments a habit
   :PROPERTIES:
   :EXPORT_HUGO_WEIGHT: auto
   :EXPORT_HUGO_SECTION: intermezzo_virtual_environments
   :EXPORT_FILE_NAME: make_virtual_environments_a_habit
   :END:

You just learned what are python virtual environment and how to use them? Don't look back, and make them a habit.
The limitation that only one version of a package can be installed at one time in one python environment can be the source
of very frustrating problems, distracting you from your research.
Moreover, using one python environment for all your projects means that this environment will change as you work on different projects,
making it very hard to resolve dependency problems when they (and they will) occur.

Most of the time, a better approach is to have one (or more if needed) virtual envirornments per analyses and projects.
Coming back to our earlier example with the ~tstools~ package used in analysis analysis1 and analysis2, a recommnded setup
would be
#+begin_example
  tstools/
	  setup.py
	  tstools
	  venv-tstools
  (venv-tstools) $ pip install -e tstools/

  analysis1/
	  analysis1.py
	  data/
	  venv-analysis1/
  (venv-analysis1) $ pip install tstools/

  analysis2/
	  analysis2.py
	  data/
	  venv-analysis2/
  (venv-analysis2) $ pip install tstools/
#+end_example

When working on the package itself, we work within the virtual environment ~venv-tstools~, in
which the package is installed in editable mode. In this way, we avoid constant re-installation
of the package each time we make a change to it.

When working on either analyses, we activate the corresponding virtual environment, in which
our package ~tstools~ is installed in normal, non-editable mode, possibly along all the
other packages that we need for this particular analysis.

#+begin_quote
Most GNU/Linux distributions as well as MacOS come with a version of python already installed.
This version is often referred to as the /system python/ or the /base python/. *Leave it alone*.
As the name suggest, this version of python is used likely to be used by some parts of your system,
and updating or breaking it would mean breaking these partsof your system that rely on it.
#+end_quote


*** TODO Installing utilities in global python 3.8                 :noexport:
*** TODO managing several versions of pytho nwith pyenv            :noexport:

** Summary
   :PROPERTIES:
   :EXPORT_HUGO_WEIGHT: auto
   :EXPORT_HUGO_SECTION: intermezzo_virtual_environments
   :EXPORT_FILE_NAME: summary
   :END:

- One big limitations of python is that only one version of a package can be installed in a given environment.
- virtual environments allow us to create multiple python environments, isolated from each other. Therefore we don't worry
  about breaking other projects that may rely on other versions of some packages.
- Having one virtual env per analysis is a good research practice since it faciliates reproducibility of your results.
- never use the system python installation, unless your have a very good reason to.

* Part 3 - Sharing the package
You now have a python package that you can use independently in your analyses.
This package lives somehwere in your system (the ~tstools/~) directory and your can install
it in a project's virtualenv using setuptools (~python setup.py install~).

We now look at ways your can /share/ your package with people interested in using your pkg.
This includes yourself.

Sharing means making it straightforward to both
- Obtain the source code
- Install and use the package

In practice this means that anyone will be able to "pip install" your package:
#+begin_src shell
pip install tstools
#+end_src

** Making tstools pip installable
*** Creating distributions
**** Building the distribution(s)
 The first is to generate a /distribution/ for the package, /i.e/ the ensemble of files and data
 necessary to both install and use the package.
 This usually takes the from of, or is akin to, an archive (~.tar~, ~.zip~).

 Make sure that you are in the ~tstools~ project root (where the ~setup.py~ is).
 #+begin_src python
 python setup.py sdist bdist_wheel
 #+end_src
 This builds *two* distribution:
 - A source distribution. It is a ~.tar~ archive containing the source (the ~.py~ files) and possibly the data required
   to use and/or test the package (/e.g/ parameters, input files, input data for tests...)
 - A wheel (or built distribution). In many cases this is very similar to the source distribution, but can present important advantages.

 We will discuss the difference between these twp distributions later.
 For now, suffice to remember that both are commonly generated together, and it is recommended
 to make both avaialable to users. Again, we'll understand why later.

 So where are these distributions? Notice that a directory ~lib/~ appeared next to the ~setup.py~
 #+begin_src shell
 ls lib/
 #+end_src

 #+NAME: install tstools with pip
 #+begin_exercise
 - Create a fresh virtualenv
 - Install tstools whith ~pip install tstools.wheelp~ (or ~pip install tstools.tar.gz~)
 #+end_exercise

**** Sharing the distribution: PyPI
 By generating the distribution(s) we bundled all the files and data required to install and use our package
 into a single file that can be very conviently installed through ~pip~.

 But we still need to make this file avaialbe to others.

 Python distributions can be freely hosted on Python Package Index (PyPI).
 Looking at numpy for instance, we can see all the available distribtions.

 When installing a python package as
 #+begin_src shell
 pip install numpy
 #+end_src
 by default ~pip~ makes a request to PyPI for the package ~numpy~ and downloads and install the
 relevant wheel.
 if no wheel availabe, it will download and install the ~sdist~.

 Let's see how to upload our tstools distributions to PyPI.

*** Uploading distributions to PyPI
 In this section we upload the source and wheel distribtuion created earlier to the test PyPI
 repository.
 For the purposes of this workshop, we actually make use of the test resposioty.
 It is a reposotory intended for tests only and there is no garantuee that your package will remain
 avaialbe on it over long duration. but it's the perfect tool to explore and learn.

 you will have to have an account on [[https://test.pypi.org/][test.pypi.org]] to continue.

 Uploading python distributions to TestPyPI (and PyPI) is made easy by a small python utility called [[https://pypi.org/project/twine/][twine]].

 First, within the ~tstools~ venv, install twine:
 #+begin_example
 $ (tstools) pip install twine
 #+end_example

 Then, upload the content of the ~lib/~ direcotry to TestPyPI

 #+begin_example
   $ (tstools) twine upload -i https://testpypi.org/simple lib/*
 #+end_example
 the switch ~-i https://testpypi.org/simple~ tells twine to upload the files to the test PyPI repo instead
 of the regular PyPI repo.

 #+NAME: Upload distributions and install tstools
 #+begin_exercise
 - In the tstools virtualenv, install twine and upload the distributions
 - Create a fresh virtualenv and install tstools from TestPyPI.
   Check the output of ~pip~ for the line(s) that show that the pacakge indeed comes from the TestPyPI index.

 Congratulations: you made your package pip installable !!
 #+end_exercise

* Part 4 - Going further
*** Source vs built distributions
*** Inlude data in the distribution
*** include tests in the distribution
*** Custom setuptools commands
*** Packaging C/C++/Fortran extensions
*** tox
* Footnotes
[fn:1] Since Python 3.3, this isn't technically true. Directories without a ~__init__.py~ file
are called namespace packages, see [[https://packaging.python.org/guides/packaging-namespace-packages/][Packaging namespace packages]] on the Python Packaging User Guide).
However, their discussion is beyond the scope of this course.
